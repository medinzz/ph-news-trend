# Philippine News Crawler and Trend Analyzer

## Overview

This project is a news crawler designed to gather and analyze the latest news information from various sources in the Philippines. By employing keyword analysis, the project aims to identify trending topics and keywords over time, offering valuable insights into public discourse and supporting informed decision-making.

## Current Status

The project is currently under development. The following milestones have been reached:


-   **Integration of the following news outlets using their API**
    -   [ABS-CBN News](https://od2-content-api.abs-cbn.com/prod/latest?sectionId=nation&brand=OD&partner=imp-01&limit=8&offset=undefined) 
    -   [Rappler](https://www.rappler.com/wp-json/wp/v2/posts)
    -   [Manila Bulletin](https://mb.com.ph/api/pb/fetch-articles-paginated?limit=4&hide_widget_in_pagination=1&page=100&section_id=25)
    <!-- -   **Inquier** - Inquirer has different subdomains for their WP API:
        - [newsinfo](https://newsinfo.inquirer.net/wp-json/wp/v2/posts)
        - [globalnation](https://globalnation.inquirer.net/wp-json/wp/v2/posts)
        - [business](https://business.inquirer.net/wp-json/wp/v2/posts)
        - [lifestyle](https://lifestyle.inquirer.net/wp-json/wp/v2/posts)
        - [entertainment](https://entertainment.inquirer.net/wp-json/wp/v2/posts)
        - [technology](https://technology.inquirer.net/wp-json/wp/v2/posts)
        - [sports](https://sports.inquirer.net/wp-json/wp/v2/posts)
        - [esports](https://esports.inquirer.net/wp-json/wp/v2/posts)
        - [opinion](https://opinion.inquirer.net/wp-json/wp/v2/posts)
        - [usa](https://usa.inquirer.net/wp-json/wp/v2/posts)
        - [bandera](https://bandera.inquirer.net/wp-json/wp/v2/posts)
        - [cebudailynews](https://cebudailynews.inquirer.net/wp-json/wp/v2/posts)
        - [pop](https://pop.inquirer.net/wp-json/wp/v2/posts) -->


## Planned Features

The next major phase of development will focus on:

-   **Keyword Analysis:** Implementing algorithms and techniques to identify significant keywords and track their frequency over time.
-   **Trend Identification:** Analyzing the keyword data to discover emerging and declining trends in Philippine news.
-   **Data Visualization:** Creating visual representations (e.g., charts, graphs) to effectively communicate the identified trends and insights.
-   **Support for More News Sources:** Expanding the crawler to include a wider range of reputable Philippine news outlets.

## Potential Applications

The insights generated by this project can be valuable for:

-   **Journalists and Researchers:** Understanding the evolving news landscape and identifying key areas of public interest.
-   **Policymakers:** Gaining insights into public discourse surrounding important issues.
-   **Businesses and Organizations:** Monitoring public sentiment and identifying emerging trends relevant to their operations.
-   **General Public:** Staying informed about the most discussed topics in the Philippines.

## Getting Involved

This project is currently being developed independently. Contributions and feedback are welcome as the project progresses. If you have expertise in web scraping, natural language processing, data analysis, or data visualization and are interested in contributing, please feel free to reach out.

## News Scraper

This news scraper supports both **SQLite** (local database) and **BigQuery** (cloud database) storage backends, allowing you to easily switch between them based on your needs.

## Features

- ✅ Fetch articles from multiple news sources (ABS-CBN, Manila Bulletin, Rappler)
- ✅ Switch between SQLite and BigQuery storage
- ✅ Unified storage interface (no code changes needed to switch backends)
- ✅ Configuration-based setup
- ✅ Command-line options for flexibility

## Project Structure

```
news/
├── apis.py                 # Article fetching functions
├── crawler.py              # Scrapy crawler for inquirer articles (old) - not working
├── items.py                # Scrapy container for inquirer articles (old)
├── pipelines.py            # Scrapy pipline for inquirer articles (old)

util/
├── storage_backend.py      # Storage backend implementations
├── bigquery.py            # BigQuery utility functions (legacy)
├── sqlite.py              # SQLite connection (legacy)
├── tools.py               # General utilities
├── const.py               # Constants (GCP_PROJECT_ID, etc.)
config.py                  # Configuration file
main.py                    # Entry point script
```

## Installation

```bash
# Install required packages
pip install aiohttp google-cloud-bigquery pandas beautifulsoup4 html2text pyarrow dotenv pandas-gbq
```

## Quick Start

### Method 1: Using Configuration File (Recommended)

1. **Edit `config.py`** to set your preferred backend:

```python
# For SQLite (default)
STORAGE_BACKEND = 'sqlite'

# For BigQuery
STORAGE_BACKEND = 'bigquery'
```

2. **Run the scraper:**

```bash
# Use default configuration (7 days back)
python main.py

# Fetch articles from a specific date
python main.py --start-date 2024-01-01

# Fetch articles from last 30 days
python main.py --days-back 30

# Show current configuration
python main.py --show-config
```

### Method 2: Using Command Line Arguments

```bash
# Override backend from command line (SQLite)
python main.py --backend sqlite

# Override backend from command line (BigQuery)
python main.py --backend bigquery

# Combine with other options
python main.py --backend bigquery --days-back 14
```

### Method 3: Using Environment Variables

```bash
# Set storage backend
export STORAGE_BACKEND=bigquery
export BQ_DATASET_ID=ph_news
export BQ_TABLE_NAME=articles

# Run scraper
python main.py
```

## Configuration Options

### SQLite Configuration

Edit `config.py`:

```python
SQLITE_CONFIG = {
    'db_path': 'articles.db',      # Path to SQLite database file
    'table_name': 'articles'        # Table name
}
```

Or use environment variables:

```bash
export STORAGE_BACKEND=sqlite
export SQLITE_DB_PATH=my_articles.db
export SQLITE_TABLE_NAME=news_articles
```

### BigQuery Configuration

Edit `config.py`:

```python
BIGQUERY_CONFIG = {
    'dataset_id': 'ph_news',     # BigQuery dataset ID
    'table_name': 'articles'        # BigQuery table name
}
```

Or use environment variables:

```bash
export STORAGE_BACKEND=bigquery
export BQ_DATASET_ID=my_news_dataset
export BQ_TABLE_NAME=news_articles
```

**Note:** Make sure to set `GCP_PROJECT_ID` in `util/const.py` for BigQuery.

## Database Schema

Both backends use the same schema:

| Column         | Type      | Description                    |
|----------------|-----------|--------------------------------|
| id             | STRING    | Unique article identifier      |
| source         | STRING    | News source (abs-cbn, etc.)    |
| url            | STRING    | Article URL                    |
| category       | STRING    | Article category               |
| subcategory    | STRING    | Article subcategory (optional) |
| title          | STRING    | Article title                  |
| author         | STRING    | Article author                 |
| date           | DATE      | Publication date               |
| publish_time   | TIMESTAMP | Publication timestamp          |
| content        | STRING    | Article content (Markdown)     |
| tags           | STRING    | Comma-separated tags           |

## Usage Examples

### Example 1: Daily Scraping to SQLite

```python
from news.apis import get_all_articles
from datetime import datetime, timedelta

# Fetch yesterday's articles
yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
get_all_articles(
    start_date=yesterday,
    backend='sqlite',
    db_path='daily_news.db'
)
```

### Example 2: Weekly Batch to BigQuery

```python
from news.apis import get_all_articles
from datetime import datetime, timedelta

# Fetch last 7 days
week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
get_all_articles(
    start_date=week_ago,
    backend='bigquery',
    dataset_id='news_archive',
    table_name='weekly_articles'
)
```

## Advanced Usage

### Creating Custom Storage Backends

You can create your own storage backend by inheriting from `StorageBackend`:

```python
from util.storage_backend import StorageBackend

class PostgreSQLBackend(StorageBackend):
    def __init__(self, connection_string: str):
        # Your initialization code
        pass
    
    def insert_record(self, item: dict) -> None:
        # Your insert logic
        pass
    
    def fetch_all(self, query: str) -> list:
        # Your fetch logic
        pass
    
    def close(self) -> None:
        # Your cleanup logic
        pass
```

### Querying Stored Data

#### SQLite

```python
from util.storage_backend import SQLiteBackend

storage = SQLiteBackend(db_path='articles.db')
results = storage.fetch_all("SELECT * FROM articles WHERE source = 'rappler' LIMIT 10")
storage.close()
```

#### BigQuery

```python
from util.storage_backend import BigQueryBackend

storage = BigQueryBackend(dataset_id='ph_news')
results = storage.fetch_all("""
    SELECT title, publish_time, source 
    FROM articles 
    WHERE DATE(publish_time) = CURRENT_DATE()
    ORDER BY publish_time DESC
""")
storage.close()
```

## Migration Guide

### Migrating from SQLite to BigQuery

1. Export data from SQLite:

```python
import pandas as pd
import sqlite3

conn = sqlite3.connect('articles.db')
df = pd.read_sql_query("SELECT * FROM articles", conn)
conn.close()
```

2. Load to BigQuery:

```python
from util.bigquery import create_or_update_table

create_or_update_table(
    df=df,
    dataset_id='ph_news',
    table_name='articles',
    mode='replace'
)
```

## Troubleshooting

### BigQuery Authentication

If you encounter authentication errors:

```bash
# Set up Google Cloud credentials
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account-key.json"

# Or authenticate using gcloud
gcloud auth application-default login
```

### SQLite Locked Database

If you get "database is locked" errors:

- Close all connections to the database
- Ensure only one process is writing at a time
- Consider using BigQuery for concurrent writes

## Performance Considerations

- **SQLite**: Fast for local development, single-user scenarios. Not suitable for concurrent writes.
- **BigQuery**: Scalable for large datasets, supports concurrent writes, better for production.

## License

MIT License
